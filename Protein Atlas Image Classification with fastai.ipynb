{"cells":[{"metadata":{"_uuid":"ed4f00b79100f07dc51484e33cf5330e0baff57c"},"cell_type":"markdown","source":"## Introduction\nThe Goal of this project is to develop a model capable of classifying mixed patterns of proteins in  microscope images. However, unlike most image labeling tasks, where binary or multiclass labeling is considered, in this competition each image can have multiple labels.  Here all image samples are represented by four filters (stored as individual files), **the protein of the protein of    interest (green) interest (green) ​​plus three cellular landmarks: ​nucleus (blue) nucleus (blue)​​, ​microtubules (red) microtubules (red)​​,​ endoplasmic endoplasmic    reticulum (yellow reticulum (yellow​​).** Therefore, An additional challenge is 4-channel input to the model (RGBY),  which is different from ones used in most of pretrained models (RGB input).   ​Images we will be using for this are generated with the help of microscope and at a far greater pace  than what can be manually evaluated. Therefore, the need is greater than ever for automating  biomedical image analysis to accelerate the understanding of human cells and disease. "},{"metadata":{"trusted":true,"_uuid":"4f837fd7bf8bb0d968b45b542007a2f08bef50d6"},"cell_type":"code","source":"#installing fastai and torchvision \n!pip install fastai==0.7.0 --no-deps\n!pip install torch==0.4.1 torchvision==0.2.1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdd2ccb5f629c5be4276205bfe5900a623a5753b"},"cell_type":"markdown","source":"## Problem Statement \nClassification of proteins has been limited to single patterns in one or a few cell types, but in order  to fully understand the complexity of the human cell, models must classify mixed patterns across a  range of different human cells. And automating the biomedical image analysis will  accelerate the  understanding of human cells and disease. "},{"metadata":{"trusted":true,"_uuid":"774147283f28a040275aa8666f7dcc8cf5678592"},"cell_type":"code","source":"# import necessary libs\nfrom fastai.conv_learner import *\nfrom fastai.dataset import *\n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nimport scipy.optimize as opt\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#load train.csv\ndata = pd.read_csv('../input/train.csv')\nprint(data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b2e43c2802e666d85199d52c48a9125fa8f092e"},"cell_type":"code","source":"# total label names \nlabel_names = {\n    0:  \"Nucleoplasm\",  \n    1:  \"Nuclear membrane\",   \n    2:  \"Nucleoli\",   \n    3:  \"Nucleoli fibrillar center\",   \n    4:  \"Nuclear speckles\",\n    5:  \"Nuclear bodies\",   \n    6:  \"Endoplasmic reticulum\",   \n    7:  \"Golgi apparatus\",   \n    8:  \"Peroxisomes\",   \n    9:  \"Endosomes\",   \n    10:  \"Lysosomes\",   \n    11:  \"Intermediate filaments\",   \n    12:  \"Actin filaments\",   \n    13:  \"Focal adhesion sites\",   \n    14:  \"Microtubules\",   \n    15:  \"Microtubule ends\",   \n    16:  \"Cytokinetic bridge\",   \n    17:  \"Mitotic spindle\",   \n    18:  \"Microtubule organizing center\",   \n    19:  \"Centrosome\",   \n    20:  \"Lipid droplets\",   \n    21:  \"Plasma membrane\",   \n    22:  \"Cell junctions\",   \n    23:  \"Mitochondria\",   \n    24:  \"Aggresome\",   \n    25:  \"Cytosol\",   \n    26:  \"Cytoplasmic bodies\",   \n    27:  \"Rods & rings\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"413c9fa149c0cab6a825900effc5ccad2a555047"},"cell_type":"code","source":"#class counts\none_hot = data.Target.str.get_dummies(sep=' ')\none_hot.columns = map(int, one_hot.columns); one_hot.head()\ncounts = one_hot.agg('sum')[:].rename(lambda x: label_names[x]).sort_values(ascending=False)\ncounts.plot('bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00663e9b6f2751c24bd2a599720a0357fbd1d519"},"cell_type":"markdown","source":"## Data Exploration\nLet's take a look at some of the images and try to imagine what we want our net to find here."},{"metadata":{"trusted":true,"_uuid":"006cad1a853747a811098bd63d01cda2c9c1b1bd"},"cell_type":"code","source":"DIR = '../input/train'\nprint(len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34ca335bd5fc420a9454ac748b8df29ff1a569a3"},"cell_type":"code","source":"import matplotlib.image as mpimg\ndef plot_images_row(img_id, ax_row):\n    filters = ['red', 'green', 'blue', 'yellow']\n    colormaps = ['Reds', 'Greens', 'Blues', 'Oranges']\n    \n    for c, ax, cmap in zip(filters, ax_row, colormaps):\n        filename = img_id + '_' + c + '.png'\n        img=mpimg.imread(os.path.join(DIR, filename))\n        imgplot = ax.imshow(img, cmap=cmap)\n\n\nid_list = data.sample(4).Id.tolist()\nfig, axes = plt.subplots(nrows=4, ncols=4)\n\nfor img_id, ax_row in zip(id_list, axes):\n    plot_images_row(img_id, ax_row)\n \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1af51d201e5ba719767f45ffdc4c19fbc0d95c6b"},"cell_type":"markdown","source":"## Data Prepration "},{"metadata":{"trusted":true,"_uuid":"017b5ca5b1bd5e2268cfc678378afd78fa2c5d64"},"cell_type":"code","source":"# lets see the image with a single colour channel (Blue)\ncolor = 'blue'\nfilename = id_list[0] + '_' + color + '.png'\nimg=mpimg.imread(os.path.join(DIR, filename))\nplt.imshow(img, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73ef4205a903fe0a6879bf59d6dbcb1ffacd9232"},"cell_type":"code","source":"#seprating our training and validation data\nlabel_csv = '../input/train.csv'\nn = len(list(open(label_csv)))-1 # no. of samples in train.csv\nval_idxs = get_cv_idxs(n) # returns the list of randomly choosen indexs that we will bw using for validation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c00096546292e04fcda2eb27864e5865db3b74b"},"cell_type":"code","source":"import pathlib\n    \ndef get_data(sz, colour): #size: pass the required size (256)  ,colour: choose any color in [red, green, blue, yellow]\n    \n         #a function that gives us the training and \n          #   validation data based on colour channel\n               \n    tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_top_down, max_zoom=1.1)       \n    data = ImageClassifierData.from_csv('../input', 'train', label_csv,bs=32, tfms=tfms,\n                    suffix='_{}.png'.format(colour), val_idxs=val_idxs, test_name='test') \n    data.path = pathlib.Path('.')\n    return data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5aac462daf7b9034f090c69f6884a05ee3d2fc1a"},"cell_type":"markdown","source":"## Building The Model\n\n* I have tried to build my model with the combination of all 4 channels (Red, Green, Blue, and Yellow), but did not get the good results. So, I decided to build my model only with the green color channel as mentioned in the competition description."},{"metadata":{"trusted":true,"_uuid":"d9640c5649ee60242b1304a0ba43795c589e6dab"},"cell_type":"code","source":"#define the model and get the data\nf_model = resnet34             #model\ndata = get_data(256, 'green')  #load the data by passing the desird colour channel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70255f81620c58aaa54ea1c2ec6dee324686f98b"},"cell_type":"code","source":"# evluation metrics: F1 score 'macro'\ndef f1(preds, targs, start=0.17, end=0.24, step=0.01):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        return max([f1_score(targs, (preds>th), average='macro')\n                    for th in np.arange(start,end,step)])\n\nmetrics=[f1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e351248433d20531be7c76741b8d19ad75ce1f4"},"cell_type":"code","source":"learn = ConvLearner.pretrained(f_model, data, ps=0.5, metrics=metrics) # creating the learner","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"765e2f438803114d32ea2274f3bf68cd4163b3b6"},"cell_type":"markdown","source":"## Hyperparameters"},{"metadata":{"trusted":true,"_uuid":"4701f4dd05ce552be42573b57b5b0e0ab2531abf"},"cell_type":"code","source":"#to find a suitable learning rate\nlrf=learn.lr_find() # function from fastai lib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7baa315ff9b445f959555c54670446e371499fa"},"cell_type":"code","source":"learn.sched.plot()  # function from fastai lib","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb96831e80a27274a99244413bbf56cbf13e712e"},"cell_type":"markdown","source":"## Training \n"},{"metadata":{"trusted":true,"_uuid":"f5f4565ee23d2538a4bccd56962981b8f6bfd729"},"cell_type":"code","source":"#unfreez the weights of the pretrained model\nlr = 0.05\nlearn.unfreeze()\nlrs=np.array([lr/10,lr/3,lr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f63dbd288b41bc142bcce9443850f48ae69ca525"},"cell_type":"code","source":"learn.fit(lrs/4, 5, cycle_len=1, cycle_mult=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2f53b0bc4cf756466c6f2803654b909a4e1c0ce"},"cell_type":"code","source":"#F1 score 'macro' on validation data\nmulti_preds, y = learn.TTA()       #using test time Augmentation \npreds = np.mean(multi_preds, 0)\nf1(preds,y) # gives the f1 score 'macro'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6882086502da97771eb3081077e8e6fb0d2322f"},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true,"_uuid":"122801b0306984df69a033f89020136b83f0d68f"},"cell_type":"code","source":"#making predictions for the testing data\npreds_t,y_t = learn.TTA(n_aug=8,is_test=True)\npreds_t = np.stack(preds_t, axis=-1)\npred_t = preds_t.mean(axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa25efb01bb2e46221a07e84a94516dee639e837"},"cell_type":"code","source":"def save_pred(pred, th=0.0, fname='Submission.csv'):\n    pred_list = []\n    for line in pred:\n        s = ' '.join(list([str(i) for i in np.nonzero(line>th)[0]]))\n        pred_list.append(s)\n    \n    df = pd.DataFrame({'Id':learn.data.test_ds.fnames,'Predicted':pred_list})\n    df.sort_values(by='Id').to_csv(fname, header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bddb2cdd50acd892a7b002b0a7b0c0183345098e"},"cell_type":"code","source":"save_pred(pred_t,0.01,'Submission.csv')\nFileLink('Submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c0b17db5a00ace12f5ef927865e6b3aeb01de18"},"cell_type":"code","source":"pd.read_csv('Submission.csv').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7c38770cdfd228b3e85abb9616d270ca5daa63c"},"cell_type":"markdown","source":"**Note** : I did this project in kaggle kernel. \n\nReference [https://github.com/GdMacmillan/kaggle-protein-classification/blob/master/notebook/EDA%20and%20baseline.ipynb](https://github.com/GdMacmillan/kaggle-protein-classification/blob/master/notebook/EDA%20and%20baseline.ipynb)"},{"metadata":{"trusted":true,"_uuid":"03061dc389d8b63b112e05bf0af8ab95f1360fe8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}